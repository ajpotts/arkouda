from __future__ import annotations

from collections import abc
from typing import (
    TYPE_CHECKING,
    Callable,
    Literal,
    cast,
    overload,
)
import warnings

import numpy as np

from pandas._config import using_copy_on_write

from pandas.util._decorators import cache_readonly
from pandas.util._exceptions import find_stack_level

from pandas.core.dtypes.common import (
    is_bool,
    is_iterator,
)
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import (
    ABCDataFrame,
    ABCSeries,
)
from pandas.core.dtypes.missing import isna

from pandas.core.arrays.categorical import (
    factorize_from_iterable,
    factorize_from_iterables,
)
import pandas.core.common as com
from pandas.core.indexes.api import (
    Index,
    MultiIndex,
    all_indexes_same,
    default_index,
    ensure_index,
    get_objs_combined_axis,
    get_unanimous_names,
)
from pandas.core.internals import concatenate_managers

if TYPE_CHECKING:
    from collections.abc import (
        Hashable,
        Iterable,
        Mapping,
    )

    from pandas._typing import (
        Axis,
        AxisInt,
        HashableT,
    )

    from pandas import (
        DataFrame,
        Series,
    )




import pandas as pd
import numpy as np
import arkouda as ak
from arkouda import Index
ak.connect()

ref_df = pd.DataFrame(
            {
                "idx": list(range(11)),
                "val_0": [0, 1, 2, 3, 4, 0, 0, 0, 0, 0, 0],
                "val_1": [0, 0, 0, 0, 0, 5, 6, 7, 8, 9, 10],
            }
        )



s = ak.Series(ak.arange(5))
s2 = ak.Series(ak.arange(5, 11), ak.arange(5, 11))
s3 = ak.Series(ak.arange(5, 10), ak.arange(5, 10))

df = ak.Series.concat([s, s2], axis=1)

i = Index(s.values)
i2 = Index(s2.values)


#################

value_labels = None
index_labels = None

if value_labels is None:
    value_labels = [f"val_{i}" for i in range(len(arrays))]

aitor = iter(arrays)
idx = next(aitor).index
idx = idx.concat([i.index for i in aitor])

data = idx.to_dict(index_labels)

dfs = []

if value_labels is not None:
    # Expect value_labels to always be not None; were doing the check for mypy
    for col, label in zip(arrays, value_labels):
        print("\n\n***\n")
        print(type(col))
        print(col)
        print(label)
        tmp_df = ak.dataframe.DataFrame({label:col.values})
        display(tmp_df)
        dfs.append(tmp_df)
        #data[str(label)] = ak.alignment.lookup(col.index.index, col.values, idx.index, fillvalue=0)

stuff = ak.dataframe.DataFrame.concat(dfs)
display(stuff)

result = ak.dataframe.DataFrame(data)

display(result)








n = pd.Series(np.arange(5))
n2 = pd.Series(np.arange(5, 11), np.arange(5, 11))
n3 = pd.Series(np.arange(5, 10), np.arange(5, 10))

df1 = pd.DataFrame({"test1":n.values})
df2 = pd.DataFrame({"test2":n2.values})


pd.concat([df1, df2])















    def _get_sample_object(
        objs: list[ak.Series | ak.DataFrame],
        ndims: set[int],
        keys,
        names,
        levels,
    ) -> tuple[ak.Series | ak.DataFrame, list[ak.Series | ak.DataFrame]]:
        # get the sample
        # want the highest ndim that we have, and must be non-empty
        # unless all objs are empty
        sample: ak.Series | ak.DataFrame | None = None
        if len(ndims) > 1:
            max_ndim = max(ndims)
            for obj in objs:
                if obj.ndim == max_ndim and np.sum(obj.shape):
                    sample = obj
                    break

        else:
            # filter out the empties if we have not multi-index possibilities
            # note to keep empty Series as it affect to result columns / name
            non_empties = [obj for obj in objs if sum(obj.shape) > 0 or obj.ndim == 1]

            if len(non_empties) and (
                keys is None and names is None and levels is None 
            ):
                objs = non_empties
                sample = objs[0]

        if sample is None:
            sample = objs[0]
        return sample, objs



test = _get_sample_object(
        dfs,
        ndims={2},
        keys={"val_1","val_2"},
        names=None,
        levels=None,
    )

display(test)








def concat(
    objs: Iterable[Series | DataFrame] | Mapping[HashableT, Series | DataFrame],
    *,
    axis: Axis = 0,
    join: str = "outer",
    ignore_index: bool = False,
    keys: Iterable[Hashable] | None = None,
    levels=None,
    names: list[HashableT] | None = None,
    verify_integrity: bool = False,
    sort: bool = False,
    copy: bool | None = None,
) -> DataFrame | Series:

    if copy is None:
        if using_copy_on_write():
            copy = False
        else:
            copy = True
    elif copy and using_copy_on_write():
        copy = False

    op = _Concatenator(
        objs,
        axis=axis,
        ignore_index=ignore_index,
        join=join,
        keys=keys,
        levels=levels,
        names=names,
        verify_integrity=verify_integrity,
        copy=copy,
        sort=sort,
    )

    return op.get_result()


test2 = concat([s,s2])

concat([ak.DataFrame(df1), ak.DataFrame(df2)])




































